[ft_instance_hyperparameter]
max_batch_size=8 ; Use for allocate the buffer
max_seq_len=2048 ; The sequence length of position embedding table, should move to model hyper-parameter
beam_width=1 ; beam width for beam search
top_k=0 ; k value for top k sampling
top_p=0.5 ; p value for top p sampling
temperature=1.0 ; Use for sampling
repetition_penalty=2.0 ; Use for sampling
len_penalty=0.0
beam_search_diversity_rate=0.0
data_type=fp16
enable_custom_all_reduce=0

tensor_para_size=1
pipeline_para_size=1

model_name=F350
model_dir=/home/iman/fpi/FasterTransformer/build/F350

[request]
request_batch_size=1 # determine by the request
request_output_len=32 # determine by the request

[F350]
weight_data_type=1
head_num=16
size_per_head=64
vocab_size=51200
decoder_layers=20
rotary_embedding=32
start_id=50256
end_id=50256
inter_size=4096
num_tasks=1 ;optional
prompt_learning_type=0 ;optional --> 0: no prompt, 1: soft_prompt, 2: prefix_prompt, 3: p/prompt_tuning

;prompt learning example (soft prompt doesn't need it)
[F350_task_0] ; task_name_id = 0
task_name=task_0
prompt_length=5
;optional
[F350_task_1] ; task_name_id = 1
task_name=task_1
prompt_length=10
